//! Storage Asset Adapter with distributed sharding and encryption
//!
//! Features:
//! - Block device management (NVMe, SSD, HDD)
//! - Distributed storage pools with replication
//! - Content-aware sharding and deduplication
//! - Encryption at rest with Kyber quantum-resistant crypto
//! - Storage health monitoring and predictive maintenance
//! - PoSpace proof validation for storage commitment

use std::collections::HashMap;
use std::sync::Arc;
use std::time::{Duration, SystemTime};
use async_trait::async_trait;
use tokio::sync::RwLock;
use serde::{Deserialize, Serialize};

use crate::assets::core::{
    AssetAdapter, AssetId, AssetType, AssetResult, AssetError,
    AssetAllocationRequest, AssetStatus, AssetState,
    PrivacyLevel, AssetAllocation, ProxyAddress,
    ResourceUsage, ResourceLimits, StorageUsage, StorageLimit,
    AdapterHealth, AdapterCapabilities, ConsensusProof,
    StorageRequirements, StorageType,
};

/// Storage allocation record
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct StorageAllocation {
    /// Asset ID
    pub asset_id: AssetId,
    /// Allocated storage devices
    pub allocated_devices: Vec<String>,
    /// Total allocated size in bytes
    pub allocated_size_bytes: u64,
    /// Storage type (SSD, NVMe, HDD, etc.)
    pub storage_type: StorageType,
    /// Replication factor
    pub replication_factor: u32,
    /// Encryption enabled
    pub encryption_enabled: bool,
    /// Encryption key ID (Kyber quantum-resistant)
    pub encryption_key_id: Option<String>,
    /// Sharding configuration
    pub sharding_config: ShardingConfig,
    /// Privacy level
    pub privacy_level: PrivacyLevel,
    /// Mount path for access
    pub mount_path: Option<String>,
    /// Allocation timestamp
    pub allocated_at: SystemTime,
    /// Last accessed timestamp
    pub last_accessed: SystemTime,
    /// Current IOPS
    pub current_iops: u32,
    /// Current throughput in MB/s
    pub current_throughput_mbps: f32,
}

/// Storage device information
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct StorageDevice {
    /// Device identifier (e.g., "/dev/nvme0n1")
    pub device_id: String,
    /// Device name/model
    pub device_name: String,
    /// Storage type
    pub storage_type: StorageType,
    /// Total capacity in bytes
    pub total_capacity_bytes: u64,
    /// Available capacity in bytes
    pub available_capacity_bytes: u64,
    /// Maximum IOPS
    pub max_iops: u32,
    /// Maximum throughput in MB/s
    pub max_throughput_mbps: u32,
    /// Serial number
    pub serial_number: String,
    /// Current status
    pub status: StorageStatus,
    /// Current allocation asset ID
    pub allocated_to: Option<AssetId>,
    /// Health metrics
    pub health_metrics: StorageHealthMetrics,
    /// SMART data
    pub smart_data: Option<SmartData>,
}

/// Storage device status
#[derive(Clone, Debug, Serialize, Deserialize)]
pub enum StorageStatus {
    /// Storage is available for allocation
    Available,
    /// Storage is allocated but idle
    Allocated,
    /// Storage is actively being used
    InUse,
    /// Storage is in maintenance mode
    Maintenance,
    /// Storage is degraded but functional
    Degraded,
    /// Storage has failed
    Failed,
}

/// Storage health metrics
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct StorageHealthMetrics {
    /// Temperature in Celsius
    pub temperature_celsius: Option<f32>,
    /// Power-on hours
    pub power_on_hours: u64,
    /// Read/write cycle count
    pub cycle_count: u64,
    /// Uncorrectable error count
    pub error_count: u64,
    /// Wear leveling count
    pub wear_level: Option<u32>,
    /// Health percentage (0-100)
    pub health_percentage: u8,
}

/// SMART data for predictive maintenance
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct SmartData {
    /// Raw read error rate
    pub read_error_rate: u64,
    /// Spin up time (for HDDs)
    pub spin_up_time: Option<u32>,
    /// Reallocated sectors count
    pub reallocated_sectors: u32,
    /// Power cycle count
    pub power_cycle_count: u64,
    /// Runtime bad blocks
    pub runtime_bad_blocks: u32,
    /// Program/erase count (for SSDs)
    pub program_erase_count: Option<u64>,
}

/// Sharding configuration for distributed storage
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct ShardingConfig {
    /// Shard size in bytes
    pub shard_size_bytes: u64,
    /// Number of shards
    pub shard_count: u32,
    /// Sharding algorithm
    pub algorithm: ShardingAlgorithm,
    /// Content-aware sharding enabled
    pub content_aware: bool,
    /// Deduplication enabled
    pub deduplication_enabled: bool,
    /// Compression enabled
    pub compression_enabled: bool,
}

/// Sharding algorithms
#[derive(Clone, Debug, Serialize, Deserialize)]
pub enum ShardingAlgorithm {
    /// Round-robin distribution
    RoundRobin,
    /// Hash-based distribution
    HashBased,
    /// Content-aware distribution
    ContentAware,
    /// RAID-like striping
    Striping,
}

/// Storage pool for distributed management
#[derive(Clone, Debug)]
pub struct StoragePool {
    /// Pool identifier
    pub pool_id: String,
    /// Total pool capacity in bytes
    pub total_capacity: u64,
    /// Available capacity in bytes
    pub available_capacity: u64,
    /// Storage type in pool
    pub storage_type: StorageType,
    /// Pool privacy level
    pub privacy_level: PrivacyLevel,
    /// Devices in pool
    pub devices: Vec<String>,
    /// Active allocations
    pub allocations: Vec<AssetId>,
    /// Pool health status
    pub health_status: PoolHealthStatus,
}

/// Storage pool health status
#[derive(Clone, Debug)]
pub enum PoolHealthStatus {
    /// Pool is healthy
    Healthy,
    /// Pool is degraded but functional
    Degraded,
    /// Pool is critical
    Critical,
    /// Pool has failed
    Failed,
}

/// Storage Asset Adapter implementation
pub struct StorageAssetAdapter {
    /// Active storage allocations by asset ID
    allocations: Arc<RwLock<HashMap<AssetId, StorageAllocation>>>,
    /// Storage device information and status
    storage_devices: Arc<RwLock<HashMap<String, StorageDevice>>>,
    /// Device allocation mapping (device_id -> asset_id)
    device_allocations: Arc<RwLock<HashMap<String, AssetId>>>,
    /// Storage pools for distributed management
    storage_pools: Arc<RwLock<HashMap<String, StoragePool>>>,
    /// Proxy address mappings
    proxy_mappings: Arc<RwLock<HashMap<ProxyAddress, AssetId>>>,
    /// Total storage capacity in bytes
    total_capacity: u64,
    /// Available storage capacity in bytes
    available_capacity: Arc<RwLock<u64>>,
    /// Storage usage statistics
    usage_stats: Arc<RwLock<StorageUsageStats>>,
}

/// Storage usage statistics
#[derive(Clone, Debug, Default)]
pub struct StorageUsageStats {
    /// Total allocations made
    pub total_allocations: u64,
    /// Total deallocations made
    pub total_deallocations: u64,
    /// Current active allocations
    pub active_allocations: u64,
    /// Total bytes allocated
    pub total_bytes_allocated: u64,
    /// Total read operations
    pub total_read_ops: u64,
    /// Total write operations
    pub total_write_ops: u64,
    /// Total bytes read
    pub total_bytes_read: u64,
    /// Total bytes written
    pub total_bytes_written: u64,
    /// Deduplication savings in bytes
    pub dedup_savings_bytes: u64,
    /// Compression savings in bytes
    pub compression_savings_bytes: u64,
}

impl StorageAssetAdapter {
    /// Create new storage adapter
    pub async fn new() -> Self {
        // Detect system storage configuration
        let (total_capacity, storage_devices) = Self::detect_storage_configuration().await;
        
        // Initialize with default storage pool
        let mut storage_pools = HashMap::new();
        storage_pools.insert("default".to_string(), StoragePool {
            pool_id: "default".to_string(),
            total_capacity,
            available_capacity: total_capacity,
            storage_type: StorageType::Ssd, // Default assumption
            privacy_level: PrivacyLevel::Private,
            devices: storage_devices.keys().cloned().collect(),
            allocations: Vec::new(),
            health_status: PoolHealthStatus::Healthy,
        });
        
        Self {
            allocations: Arc::new(RwLock::new(HashMap::new())),
            storage_devices: Arc::new(RwLock::new(storage_devices)),
            device_allocations: Arc::new(RwLock::new(HashMap::new())),
            storage_pools: Arc::new(RwLock::new(storage_pools)),
            proxy_mappings: Arc::new(RwLock::new(HashMap::new())),
            total_capacity,
            available_capacity: Arc::new(RwLock::new(total_capacity)),
            usage_stats: Arc::new(RwLock::new(StorageUsageStats::default())),
        }
    }
    
    /// Detect system storage configuration
    async fn detect_storage_configuration() -> (u64, HashMap<String, StorageDevice>) {
        // TODO: Implement actual storage detection using udev, lsblk, or similar
        // For now, simulate a reasonable configuration
        let mut storage_devices = HashMap::new();
        let mut total_capacity = 0u64;
        
        // Simulate NVMe device
        let nvme_capacity = 1024 * 1024 * 1024 * 1024; // 1TB
        storage_devices.insert("/dev/nvme0n1".to_string(), StorageDevice {
            device_id: "/dev/nvme0n1".to_string(),
            device_name: "Samsung SSD 980 PRO 1TB".to_string(),
            storage_type: StorageType::Nvme,
            total_capacity_bytes: nvme_capacity,
            available_capacity_bytes: nvme_capacity,
            max_iops: 1000000,
            max_throughput_mbps: 7000,
            serial_number: "S5GXNX0T000001".to_string(),
            status: StorageStatus::Available,
            allocated_to: None,
            health_metrics: StorageHealthMetrics {
                temperature_celsius: Some(45.0),
                power_on_hours: 1200,
                cycle_count: 15000,
                error_count: 0,
                wear_level: Some(95),
                health_percentage: 98,
            },
            smart_data: Some(SmartData {
                read_error_rate: 0,
                spin_up_time: None,
                reallocated_sectors: 0,
                power_cycle_count: 150,
                runtime_bad_blocks: 0,
                program_erase_count: Some(15000),
            }),
        });
        total_capacity += nvme_capacity;
        
        // Simulate SSD device
        let ssd_capacity = 2 * 1024 * 1024 * 1024 * 1024; // 2TB
        storage_devices.insert("/dev/sda".to_string(), StorageDevice {
            device_id: "/dev/sda".to_string(),
            device_name: "Crucial MX4 2TB".to_string(),
            storage_type: StorageType::Ssd,
            total_capacity_bytes: ssd_capacity,
            available_capacity_bytes: ssd_capacity,
            max_iops: 95000,
            max_throughput_mbps: 560,
            serial_number: "CT2000MX500SSD1".to_string(),
            status: StorageStatus::Available,
            allocated_to: None,
            health_metrics: StorageHealthMetrics {
                temperature_celsius: Some(40.0),
                power_on_hours: 2500,
                cycle_count: 25000,
                error_count: 0,
                wear_level: Some(90),
                health_percentage: 95,
            },
            smart_data: Some(SmartData {
                read_error_rate: 0,
                spin_up_time: None,
                reallocated_sectors: 0,
                power_cycle_count: 200,
                runtime_bad_blocks: 0,
                program_erase_count: Some(25000),
            }),
        });
        total_capacity += ssd_capacity;
        
        (total_capacity, storage_devices)
    }
    
    /// Allocate storage from devices
    async fn allocate_storage_from_devices(
        &self,
        storage_req: &StorageRequirements,
        asset_id: &AssetId,
    ) -> AssetResult<(Vec<String>, u64)> {
        let mut devices = self.storage_devices.write().await;
        let mut device_allocations = self.device_allocations.write().await;
        let mut allocated_devices = Vec::new();
        let mut total_allocated_size = 0u64;
        
        // Find devices matching storage type
        let mut suitable_devices: Vec<String> = devices
            .iter()
            .filter(|(_, device)| {
                matches!(device.status, StorageStatus::Available) &&
                device.storage_type == storage_req.storage_type &&
                device.available_capacity_bytes >= storage_req.size_bytes &&
                device.max_iops >= storage_req.min_iops.unwrap_or(0) &&
                device.max_throughput_mbps >= storage_req.min_bandwidth_mbps.unwrap_or(0)
            })
            .map(|(device_id, _)| device_id.clone())
            .collect();
        
        // Sort by available capacity (largest first)
        suitable_devices.sort_by_key(|device_id| {
            let device = devices.get(device_id).unwrap();
            std::cmp::Reverse(device.available_capacity_bytes)
        });
        
        // Allocate storage with replication
        let size_per_replica = storage_req.size_bytes;
        let required_replicas = storage_req.durability_replicas;
        
        if suitable_devices.len() < required_replicas as usize {
            return Err(AssetError::AllocationFailed {
                reason: format!(
                    "Insufficient storage devices for replication: {} required, {} available",
                    required_replicas, suitable_devices.len()
                )
            });
        }
        
        // Allocate to multiple devices for replication
        for device_id in suitable_devices.iter().take(required_replicas as usize) {
            let device = devices.get_mut(device_id).unwrap();
            
            if device.available_capacity_bytes < size_per_replica {
                continue; // Skip if insufficient space
            }
            
            device.status = StorageStatus::Allocated;
            device.allocated_to = Some(asset_id.clone());
            device.available_capacity_bytes -= size_per_replica;
            
            device_allocations.insert(device_id.clone(), asset_id.clone());
            allocated_devices.push(device_id.clone());
            total_allocated_size += size_per_replica;
        }
        
        if allocated_devices.len() < required_replicas as usize {
            // Rollback partial allocation
            for device_id in &allocated_devices {
                let device = devices.get_mut(device_id).unwrap();
                device.status = StorageStatus::Available;
                device.allocated_to = None;
                device.available_capacity_bytes += size_per_replica;
                device_allocations.remove(device_id);
            }
            
            return Err(AssetError::AllocationFailed {
                reason: "Insufficient storage capacity across available devices".to_string()
            });
        }
        
        Ok((allocated_devices, total_allocated_size))
    }
    
    /// Generate proxy address for storage access
    async fn generate_proxy_address(asset_id: &AssetId) -> ProxyAddress {
        let uuid_bytes = asset_id.uuid.as_bytes();
        let mut node_id = [0u8; 8];
        node_id.copy_from_slice(&uuid_bytes[..8]);
        ProxyAddress::new(
            [0x2a, 0x01, 0x04, 0xf8, 0x01, 0x10, 0x53, 0xad,
             0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x01],
            node_id,
            8080
        )
    }
    
    /// Create Kyber encryption key for quantum-resistant security
    async fn create_kyber_encryption_key(&self) -> String {
        // TODO: Implement actual Kyber key generation
        // For now, return placeholder key ID
        format!("kyber_key_{}", uuid::Uuid::new_v4())
    }
    
    /// Configure sharding for storage allocation
    async fn configure_sharding(&self, size_bytes: u64, device_count: u32) -> ShardingConfig {
        // Calculate optimal shard size (aim for 64MB shards)
        let target_shard_size = 64 * 1024 * 1024; // 64MB
        let shard_count = (size_bytes / target_shard_size).max(1) as u32;
        let actual_shard_size = size_bytes / shard_count as u64;
        
        ShardingConfig {
            shard_size_bytes: actual_shard_size,
            shard_count,
            algorithm: if device_count > 1 {
                ShardingAlgorithm::Striping
            } else {
                ShardingAlgorithm::ContentAware
            },
            content_aware: true,
            deduplication_enabled: true,
            compression_enabled: true,
        }
    }
    
    /// Update usage statistics
    async fn update_usage_stats(&self, operation: StorageOperation, bytes: u64) {
        let mut stats = self.usage_stats.write().await;
        
        match operation {
            StorageOperation::Allocate => {
                stats.total_allocations += 1;
                stats.active_allocations += 1;
                stats.total_bytes_allocated += bytes;
            },
            StorageOperation::Deallocate => {
                stats.total_deallocations += 1;
                stats.active_allocations = stats.active_allocations.saturating_sub(1);
                stats.total_bytes_allocated = stats.total_bytes_allocated.saturating_sub(bytes);
            },
            StorageOperation::Read => {
                stats.total_read_ops += 1;
                stats.total_bytes_read += bytes;
            },
            StorageOperation::Write => {
                stats.total_write_ops += 1;
                stats.total_bytes_written += bytes;
            },
        }
    }
}

/// Storage operations for statistics
#[derive(Clone, Debug)]
enum StorageOperation {
    Allocate,
    Deallocate,
    Read,
    Write,
}

#[async_trait]
impl AssetAdapter for StorageAssetAdapter {
    fn asset_type(&self) -> AssetType {
        AssetType::Storage
    }
    
    async fn validate_consensus_proof(&self, proof: &ConsensusProof) -> AssetResult<bool> {
        // Validate all four proofs with CRITICAL PoSpace validation for storage
        let valid = proof.validate().await?;
        
        if !valid {
            return Ok(false);
        }
        
        // Storage-specific validation - CRITICAL PoSpace validation
        // PoSpace: MOST IMPORTANT for storage - validate actual storage commitment
        if proof.proof_of_space.committed_space == 0 {
            return Ok(false);
        }
        
        // Verify storage location and network position
        if proof.proof_of_space.storage_location.is_empty() {
            return Ok(false);
        }
        
        // PoStake: Validate storage access stake
        if proof.proof_of_stake.stake_amount < 75 { // Moderate minimum for storage
            return Ok(false);
        }
        
        // PoWork: Validate computational work for storage management
        if proof.proof_of_work.difficulty < 14 { // Medium difficulty for storage
            return Ok(false);
        }
        
        // PoTime: Validate temporal ordering for storage operations
        let time_valid = proof.proof_of_time.logical_timestamp > 0;
        
        Ok(time_valid)
    }
    
    async fn allocate_asset(&self, request: &AssetAllocationRequest) -> AssetResult<AssetAllocation> {
        // Validate consensus proof first
        if !self.validate_consensus_proof(&request.consensus_proof).await? {
            return Err(AssetError::ConsensusValidationFailed {
                reason: "Storage allocation consensus validation failed".to_string()
            });
        }
        
        // Get storage requirements
        let storage_req = request.requested_resources.storage.as_ref()
            .ok_or_else(|| AssetError::AllocationFailed {
                reason: "No storage requirements specified".to_string()
            })?;
        
        // Check available capacity
        let available = *self.available_capacity.read().await;
        let required_capacity = storage_req.size_bytes * storage_req.durability_replicas as u64;
        
        if available < required_capacity {
            return Err(AssetError::AllocationFailed {
                reason: format!(
                    "Insufficient storage capacity: {} bytes required ({}x replication), {} available",
                    required_capacity, storage_req.durability_replicas, available
                )
            });
        }
        
        // Create asset ID
        let asset_id = AssetId::new(AssetType::Storage);
        
        // Allocate storage from devices
        let (allocated_devices, allocated_size) = self.allocate_storage_from_devices(storage_req, &asset_id).await?;
        
        // Generate proxy address
        let proxy_address = Self::generate_proxy_address(&asset_id).await;
        
        // Create encryption key for quantum security
        let encryption_key_id = if matches!(request.privacy_level, PrivacyLevel::Private | PrivacyLevel::PrivateNetwork) {
            Some(self.create_kyber_encryption_key().await)
        } else {
            None
        };
        
        // Configure sharding
        let sharding_config = self.configure_sharding(storage_req.size_bytes, allocated_devices.len() as u32).await;
        
        // Create storage allocation record
        let allocation = StorageAllocation {
            asset_id: asset_id.clone(),
            allocated_devices: allocated_devices.clone(),
            allocated_size_bytes: allocated_size,
            storage_type: storage_req.storage_type.clone(),
            privacy_level: request.privacy_level.clone(),
            replication_factor: storage_req.durability_replicas,
            encryption_enabled: encryption_key_id.is_some(),
            encryption_key_id,
            sharding_config,
            mount_path: None, // Will be assigned when mounted
            allocated_at: SystemTime::now(),
            last_accessed: SystemTime::now(),
            current_iops: 0,
            current_throughput_mbps: 0.0,
        };
        
        // Store allocation and proxy mapping
        {
            let mut allocations = self.allocations.write().await;
            allocations.insert(asset_id.clone(), allocation);
        }
        
        {
            let mut proxy_mappings = self.proxy_mappings.write().await;
            proxy_mappings.insert(proxy_address.clone(), asset_id.clone());
        }
        
        // Update available capacity
        {
            let mut available = self.available_capacity.write().await;
            *available -= allocated_size;
        }
        
        // Update usage statistics
        self.update_usage_stats(StorageOperation::Allocate, allocated_size).await;
        
        Ok(AssetAllocation {
            asset_id: asset_id.clone(),
            status: AssetStatus {
                asset_id: asset_id.clone(),
                state: AssetState::Allocated,
                allocated_at: SystemTime::now(),
                last_accessed: SystemTime::now(),
                resource_usage: ResourceUsage {
                    cpu_usage: None,
                    gpu_usage: None,
                    memory_usage: None,
                    storage_usage: None,
                    network_usage: None,
                    measurement_timestamp: SystemTime::now(),
                },
                privacy_level: PrivacyLevel::Private,
                proxy_address: None,
                consensus_proofs: Vec::new(),
                owner_certificate_fingerprint: request.certificate_fingerprint.clone(),
                metadata: HashMap::new(),
                health_status: crate::assets::core::status::AssetHealthStatus::default(),
                performance_metrics: crate::assets::core::status::AssetPerformanceMetrics::default(),
            },
            allocation_config: crate::assets::core::privacy::AllocationConfig {
                privacy_level: request.privacy_level.clone(),
                resource_allocation: crate::assets::core::privacy::ResourceAllocationConfig::default(),
                concurrency_limits: crate::assets::core::privacy::ConcurrencyLimits::default(),
                duration_config: crate::assets::core::privacy::DurationConfig::default(),
                consensus_requirements: crate::assets::core::privacy::ConsensusRequirements::default(),
            },
            access_config: crate::assets::core::privacy::AccessConfig {
                allowed_certificates: vec![request.certificate_fingerprint.clone()],
                allowed_networks: Vec::new(),
                permissions: crate::assets::core::privacy::AccessPermissions::default(),
                rate_limits: crate::assets::core::privacy::RateLimits::default(),
                auth_requirements: crate::assets::core::privacy::AuthRequirements::default(),
            },
            allocated_at: SystemTime::now(),
            expires_at: request.duration_limit.map(|d| SystemTime::now() + d),
        })
    }
    
    async fn deallocate_asset(&self, asset_id: &AssetId) -> AssetResult<()> {
        // Get allocation record
        let allocation = {
            let mut allocations = self.allocations.write().await;
            allocations.remove(asset_id)
                .ok_or_else(|| AssetError::AssetNotFound {
                    asset_id: asset_id.to_string()
                })?
        };
        
        // Free storage devices
        {
            let mut devices = self.storage_devices.write().await;
            let mut device_allocations = self.device_allocations.write().await;
            
            let size_per_device = allocation.allocated_size_bytes / allocation.allocated_devices.len() as u64;
            
            for device_id in &allocation.allocated_devices {
                if let Some(device) = devices.get_mut(device_id) {
                    device.status = StorageStatus::Available;
                    device.allocated_to = None;
                    device.available_capacity_bytes += size_per_device;
                }
                device_allocations.remove(device_id);
            }
        }
        
        // Remove proxy mapping
        {
            let mut proxy_mappings = self.proxy_mappings.write().await;
            proxy_mappings.retain(|_, mapped_asset_id| mapped_asset_id != asset_id);
        }
        
        // Update available capacity
        {
            let mut available = self.available_capacity.write().await;
            *available += allocation.allocated_size_bytes;
        }
        
        // Update usage statistics
        self.update_usage_stats(StorageOperation::Deallocate, allocation.allocated_size_bytes).await;
        
        tracing::info!(
            "Deallocated storage asset: {} ({} devices, {} bytes)", 
            asset_id, 
            allocation.allocated_devices.len(),
            allocation.allocated_size_bytes
        );
        Ok(())
    }
    
    async fn get_asset_status(&self, asset_id: &AssetId) -> AssetResult<AssetStatus> {
        let allocations = self.allocations.read().await;
        let allocation = allocations.get(asset_id)
            .ok_or_else(|| AssetError::AssetNotFound {
                asset_id: asset_id.to_string()
            })?;
        
        Ok(AssetStatus {
            asset_id: asset_id.clone(),
            state: AssetState::InUse,
            allocated_at: allocation.allocated_at,
            last_accessed: allocation.last_accessed,
            privacy_level: allocation.privacy_level.clone(),
            proxy_address: None, // Will be filled by proxy resolver
            resource_usage: self.get_resource_usage(asset_id).await?,
            consensus_proofs: Vec::new(),
            owner_certificate_fingerprint: "storage-adapter".to_string(),
            health_status: crate::assets::core::status::AssetHealthStatus::default(),
            performance_metrics: crate::assets::core::status::AssetPerformanceMetrics::default(),
            metadata: {
                let mut metadata = HashMap::new();
                metadata.insert("allocated_size_bytes".to_string(), allocation.allocated_size_bytes.to_string());
                metadata.insert("storage_type".to_string(), format!("{:?}", allocation.storage_type));
                metadata.insert("devices".to_string(), allocation.allocated_devices.len().to_string());
                metadata.insert("replication_factor".to_string(), allocation.replication_factor.to_string());
                metadata.insert("encryption_enabled".to_string(), allocation.encryption_enabled.to_string());
                metadata.insert("current_iops".to_string(), allocation.current_iops.to_string());
                metadata.insert("current_throughput_mbps".to_string(), allocation.current_throughput_mbps.to_string());
                metadata.insert("shard_count".to_string(), allocation.sharding_config.shard_count.to_string());
                metadata
            },
        })
    }
    
    async fn configure_privacy_level(&self, asset_id: &AssetId, privacy: PrivacyLevel) -> AssetResult<()> {
        let mut allocations = self.allocations.write().await;
        let allocation = allocations.get_mut(asset_id)
            .ok_or_else(|| AssetError::AssetNotFound {
                asset_id: asset_id.to_string()
            })?;
        
        allocation.privacy_level = privacy.clone();
        
        // Update encryption based on privacy level
        if matches!(privacy, PrivacyLevel::Private | PrivacyLevel::PrivateNetwork) && allocation.encryption_key_id.is_none() {
            allocation.encryption_key_id = Some(self.create_kyber_encryption_key().await);
            allocation.encryption_enabled = true;
        }
        
        tracing::info!("Updated privacy level for storage asset {}: {:?}", asset_id, privacy);
        Ok(())
    }
    
    async fn assign_proxy_address(&self, asset_id: &AssetId) -> AssetResult<ProxyAddress> {
        let proxy_address = Self::generate_proxy_address(asset_id).await;
        
        // Find existing proxy address
        let proxy_mappings = self.proxy_mappings.read().await;
        for (proxy_addr, mapped_asset_id) in proxy_mappings.iter() {
            if mapped_asset_id == asset_id {
                return Ok(proxy_addr.clone());
            }
        }
        
        Ok(proxy_address)
    }
    
    async fn resolve_proxy_address(&self, proxy_addr: &ProxyAddress) -> AssetResult<AssetId> {
        let proxy_mappings = self.proxy_mappings.read().await;
        proxy_mappings.get(proxy_addr)
            .cloned()
            .ok_or_else(|| AssetError::ProxyResolutionFailed {
                address: proxy_addr.clone()
            })
    }
    
    async fn get_resource_usage(&self, asset_id: &AssetId) -> AssetResult<ResourceUsage> {
        let allocations = self.allocations.read().await;
        let allocation = allocations.get(asset_id)
            .ok_or_else(|| AssetError::AssetNotFound {
                asset_id: asset_id.to_string()
            })?;
        
        // TODO: Implement actual storage usage monitoring
        let storage_usage = StorageUsage {
            used_bytes: allocation.allocated_size_bytes,
            total_bytes: allocation.allocated_size_bytes,
            read_iops: allocation.current_iops / 2, // Assume 50/50 read/write split
            write_iops: allocation.current_iops / 2,
            read_mbps: allocation.current_throughput_mbps / 2.0,
            write_mbps: allocation.current_throughput_mbps / 2.0,
        };
        
        Ok(ResourceUsage {
            cpu_usage: None,
            gpu_usage: None,
            memory_usage: None,
            storage_usage: Some(storage_usage),
            network_usage: None,
            measurement_timestamp: SystemTime::now(),
        })
    }
    
    async fn set_resource_limits(&self, asset_id: &AssetId, limits: ResourceLimits) -> AssetResult<()> {
        if let Some(storage_limit) = limits.storage_limit {
            tracing::info!(
                "Set storage limits for asset {}: max {} bytes, max {} IOPS, max {} MB/s",
                asset_id,
                storage_limit.max_bytes,
                storage_limit.max_iops,
                storage_limit.max_bandwidth_mbps
            );
        }
        Ok(())
    }
    
    async fn health_check(&self) -> AssetResult<AdapterHealth> {
        let stats = self.usage_stats.read().await;
        let devices = self.storage_devices.read().await;
        let available = *self.available_capacity.read().await;
        
        let failed_devices = devices.values().filter(|device| matches!(device.status, StorageStatus::Failed)).count();
        let degraded_devices = devices.values().filter(|device| matches!(device.status, StorageStatus::Degraded)).count();
        let healthy = failed_devices == 0 && degraded_devices < 2 && available > 0;
        
        let average_health = devices.values()
            .map(|d| d.health_metrics.health_percentage as f64)
            .sum::<f64>() / devices.len() as f64;
        
        let mut performance_metrics = HashMap::new();
        performance_metrics.insert("total_capacity_gb".to_string(), (self.total_capacity / (1024 * 1024 * 1024)) as f64);
        performance_metrics.insert("available_capacity_gb".to_string(), (available / (1024 * 1024 * 1024)) as f64);
        performance_metrics.insert("capacity_utilization_percent".to_string(), 
            ((self.total_capacity - available) as f64 / self.total_capacity as f64) * 100.0);
        performance_metrics.insert("active_allocations".to_string(), stats.active_allocations as f64);
        performance_metrics.insert("total_devices".to_string(), devices.len() as f64);
        performance_metrics.insert("failed_devices".to_string(), failed_devices as f64);
        performance_metrics.insert("degraded_devices".to_string(), degraded_devices as f64);
        performance_metrics.insert("average_health_percent".to_string(), average_health);
        performance_metrics.insert("dedup_savings_gb".to_string(), (stats.dedup_savings_bytes / (1024 * 1024 * 1024)) as f64);
        
        Ok(AdapterHealth {
            healthy,
            message: if healthy {
                "Storage adapter operating normally".to_string()
            } else {
                format!("Storage adapter issues: {} failed, {} degraded devices", failed_devices, degraded_devices)
            },
            last_check: SystemTime::now(),
            performance_metrics,
        })
    }
    
    fn get_capabilities(&self) -> AdapterCapabilities {
        AdapterCapabilities {
            asset_type: AssetType::Storage,
            supported_privacy_levels: vec![
                PrivacyLevel::Private,
                PrivacyLevel::PrivateNetwork,
                PrivacyLevel::P2P,
                PrivacyLevel::PublicNetwork,
                PrivacyLevel::FullPublic,
            ],
            supports_proxy_addressing: true,
            supports_resource_monitoring: true,
            supports_dynamic_limits: true,
            max_concurrent_allocations: Some(100),
            features: vec![
                "distributed_storage".to_string(),
                "replication".to_string(),
                "sharding".to_string(),
                "deduplication".to_string(),
                "compression".to_string(),
                "kyber_encryption".to_string(),
                "health_monitoring".to_string(),
                "smart_data".to_string(),
                "predictive_maintenance".to_string(),
                "content_aware_sharding".to_string(),
            ],
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::assets::core::{SpaceProof, StakeProof, WorkProof, TimeProof, WorkloadType, WorkState};
    
    async fn create_test_storage_request() -> AssetAllocationRequest {
        AssetAllocationRequest {
            asset_type: AssetType::Storage,
            requested_resources: crate::assets::core::ResourceRequirements {
                storage: Some(StorageRequirements {
                    size_bytes: 100 * 1024 * 1024 * 1024, // 100GB
                    storage_type: StorageType::Ssd,
                    min_iops: Some(1000),
                    min_bandwidth_mbps: Some(100),
                    durability_replicas: 2,
                }),
                ..Default::default()
            },
            privacy_level: PrivacyLevel::Private,
            consensus_proof: ConsensusProof::new(
                SpaceProof {
                    node_id: "test-node".to_string(),
                    storage_path: "/test/storage".to_string(),
                    allocated_size: 100 * 1024 * 1024 * 1024,
                    proof_hash: vec![1, 2, 3, 4],
                    timestamp: SystemTime::now(),
                },
                StakeProof {
                    stake_holder: "test-holder".to_string(),
                    stake_holder_id: "test-holder-id".to_string(),
                    stake_amount: 100,
                    stake_timestamp: SystemTime::now(),
                },
                WorkProof {
                    worker_id: "test-worker".to_string(),
                    workload_id: "test-workload".to_string(),
                    process_id: 12345,
                    computational_power: 50,
                    workload_type: WorkloadType::Storage,
                    work_state: WorkState::Completed,
                },
                TimeProof {
                    network_time_offset: Duration::from_secs(30),
                    time_verification_timestamp: SystemTime::now(),
                    nonce: 42,
                    proof_hash: vec![5, 6, 7, 8],
                },
            ),
            certificate_fingerprint: "test-cert".to_string(),
        }
    }
    
    #[tokio::test]
    async fn test_storage_adapter_creation() {
        let adapter = StorageAssetAdapter::new().await;
        assert_eq!(adapter.asset_type(), AssetType::Storage);
        assert!(adapter.total_capacity > 0);
    }
    
    #[tokio::test]
    async fn test_storage_allocation() {
        let adapter = StorageAssetAdapter::new().await;
        let request = create_test_storage_request().await;
        
        let allocation = adapter.allocate_asset(&request).await.unwrap();
        assert_eq!(allocation.asset_id.asset_type, AssetType::Storage);
        
        // Test deallocation
        adapter.deallocate_asset(&allocation.asset_id).await.unwrap();
    }
    
    #[tokio::test]
    async fn test_storage_health_check() {
        let adapter = StorageAssetAdapter::new().await;
        let health = adapter.health_check().await.unwrap();
        
        assert!(health.healthy);
        assert!(health.performance_metrics.contains_key("total_capacity_gb"));
        assert!(health.performance_metrics.contains_key("average_health_percent"));
    }
    
    #[tokio::test]
    async fn test_storage_capabilities() {
        let adapter = StorageAssetAdapter::new().await;
        let capabilities = adapter.get_capabilities();
        
        assert_eq!(capabilities.asset_type, AssetType::Storage);
        assert!(capabilities.supports_proxy_addressing);
        assert!(capabilities.features.contains(&"distributed_storage".to_string()));
        assert!(capabilities.features.contains(&"kyber_encryption".to_string()));
    }
}