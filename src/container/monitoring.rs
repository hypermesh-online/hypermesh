//! Container monitoring and metrics collection

use super::{
    lifecycle::ContainerId,
    error::Result,
};
use async_trait::async_trait;
use serde::{Serialize, Deserialize};
use std::collections::HashMap;
use std::time::{Duration, SystemTime, Instant};
use tracing::{info, debug};

/// Container metrics
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ContainerMetrics {
    pub container_id: ContainerId,
    pub timestamp: SystemTime,
    pub cpu_usage_percent: f64,
    pub memory_usage_bytes: u64,
    pub memory_limit_bytes: u64,
    pub network_rx_bytes: u64,
    pub network_tx_bytes: u64,
    pub filesystem_read_bytes: u64,
    pub filesystem_write_bytes: u64,
    pub processes: u32,
    pub file_descriptors: u32,
    pub uptime_seconds: u64,
}

/// Performance counters
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PerformanceCounters {
    pub context_switches: u64,
    pub page_faults: u64,
    pub system_calls: u64,
    pub interrupts: u64,
}

/// Container monitor trait
#[async_trait]
pub trait ContainerMonitor: Send + Sync {
    async fn start_monitoring(&self, id: ContainerId) -> Result<()>;
    async fn stop_monitoring(&self, id: ContainerId) -> Result<()>;
    async fn get_metrics(&self, id: ContainerId) -> Result<ContainerMetrics>;
    async fn get_metrics_history(&self, id: ContainerId, duration: Duration) -> Result<Vec<ContainerMetrics>>;
    async fn get_performance_counters(&self, id: ContainerId) -> Result<PerformanceCounters>;
    async fn set_alert_threshold(&self, id: ContainerId, metric: String, threshold: f64) -> Result<()>;
}

/// Default container monitor implementation
pub struct DefaultContainerMonitor {
    monitoring: std::sync::Arc<tokio::sync::RwLock<HashMap<ContainerId, MonitoringSession>>>,
    metrics_history: std::sync::Arc<tokio::sync::RwLock<HashMap<ContainerId, Vec<ContainerMetrics>>>>,
}

/// Monitoring session
#[derive(Debug)]
struct MonitoringSession {
    container_id: ContainerId,
    started_at: Instant,
    collection_interval: Duration,
    alert_thresholds: HashMap<String, f64>,
}

impl DefaultContainerMonitor {
    pub fn new() -> Self {
        Self {
            monitoring: std::sync::Arc::new(tokio::sync::RwLock::new(HashMap::new())),
            metrics_history: std::sync::Arc::new(tokio::sync::RwLock::new(HashMap::new())),
        }
    }
    
    /// Simulate collecting metrics from cgroups/proc filesystem
    async fn collect_metrics(&self, id: ContainerId) -> ContainerMetrics {
        let monitoring = self.monitoring.read().await;
        let uptime = monitoring.get(&id)
            .map(|session| session.started_at.elapsed().as_secs())
            .unwrap_or(0);
        
        // Simulate realistic metrics
        ContainerMetrics {
            container_id: id,
            timestamp: SystemTime::now(),
            cpu_usage_percent: 25.0 + ((rand::random::<u64>() % 100) as f64 / 10.0),
            memory_usage_bytes: 100 * 1024 * 1024 + (rand::random::<u64>() % (50 * 1024 * 1024)),
            memory_limit_bytes: 1024 * 1024 * 1024, // 1GB
            network_rx_bytes: uptime * 1024 * 10, // ~10KB/s
            network_tx_bytes: uptime * 1024 * 5,  // ~5KB/s
            filesystem_read_bytes: uptime * 1024 * 20, // ~20KB/s
            filesystem_write_bytes: uptime * 1024 * 10, // ~10KB/s
            processes: 3 + ((rand::random::<u64>() % 5) as u32),
            file_descriptors: 50 + ((rand::random::<u64>() % 100) as u32),
            uptime_seconds: uptime,
        }
    }
    
    /// Store metrics in history
    async fn store_metrics(&self, metrics: ContainerMetrics) {
        let mut history = self.metrics_history.write().await;
        let container_history = history.entry(metrics.container_id).or_insert_with(Vec::new);
        
        container_history.push(metrics);
        
        // Keep only last 1000 entries to prevent unbounded growth
        if container_history.len() > 1000 {
            container_history.remove(0);
        }
    }
}

#[async_trait]
impl ContainerMonitor for DefaultContainerMonitor {
    async fn start_monitoring(&self, id: ContainerId) -> Result<()> {
        let session = MonitoringSession {
            container_id: id,
            started_at: Instant::now(),
            collection_interval: Duration::from_secs(10),
            alert_thresholds: HashMap::new(),
        };
        
        let mut monitoring = self.monitoring.write().await;
        monitoring.insert(id, session);
        
        // Start background metrics collection task
        let monitor = self.clone();
        tokio::spawn(async move {
            let mut interval = tokio::time::interval(Duration::from_secs(10));
            loop {
                interval.tick().await;
                
                // Check if still monitoring
                let still_monitoring = {
                    let monitoring = monitor.monitoring.read().await;
                    monitoring.contains_key(&id)
                };
                
                if !still_monitoring {
                    break;
                }
                
                // Collect and store metrics
                let metrics = monitor.collect_metrics(id).await;
                monitor.store_metrics(metrics).await;
            }
        });
        
        info!("Started monitoring container {}", id);
        Ok(())
    }
    
    async fn stop_monitoring(&self, id: ContainerId) -> Result<()> {
        let mut monitoring = self.monitoring.write().await;
        monitoring.remove(&id);
        
        info!("Stopped monitoring container {}", id);
        Ok(())
    }
    
    async fn get_metrics(&self, id: ContainerId) -> Result<ContainerMetrics> {
        // Return latest metrics
        Ok(self.collect_metrics(id).await)
    }
    
    async fn get_metrics_history(&self, id: ContainerId, duration: Duration) -> Result<Vec<ContainerMetrics>> {
        let history = self.metrics_history.read().await;
        if let Some(container_history) = history.get(&id) {
            let cutoff_time = SystemTime::now() - duration;
            let filtered: Vec<_> = container_history.iter()
                .filter(|metrics| metrics.timestamp >= cutoff_time)
                .cloned()
                .collect();
            Ok(filtered)
        } else {
            Ok(Vec::new())
        }
    }
    
    async fn get_performance_counters(&self, _id: ContainerId) -> Result<PerformanceCounters> {
        // Simulate performance counter data
        Ok(PerformanceCounters {
            context_switches: 10000 + (rand::random::<u64>() % 5000),
            page_faults: 1000 + (rand::random::<u64>() % 500),
            system_calls: 50000 + (rand::random::<u64>() % 10000),
            interrupts: 5000 + (rand::random::<u64>() % 1000),
        })
    }
    
    async fn set_alert_threshold(&self, id: ContainerId, metric: String, threshold: f64) -> Result<()> {
        let mut monitoring = self.monitoring.write().await;
        if let Some(session) = monitoring.get_mut(&id) {
            session.alert_thresholds.insert(metric.clone(), threshold);
            debug!("Set alert threshold for container {} metric {}: {}", id, metric, threshold);
        }
        Ok(())
    }
}

impl Default for DefaultContainerMonitor {
    fn default() -> Self {
        Self::new()
    }
}

impl Clone for DefaultContainerMonitor {
    fn clone(&self) -> Self {
        Self {
            monitoring: Arc::clone(&self.monitoring),
            metrics_history: Arc::clone(&self.metrics_history),
        }
    }
}

// Simple random number generation for simulation
mod rand {
    use std::sync::atomic::{AtomicU64, Ordering};
    
    static SEED: AtomicU64 = AtomicU64::new(1);
    
    pub fn random<T>() -> T 
    where 
        T: From<u64>
    {
        let seed = SEED.load(Ordering::Relaxed);
        let new_seed = seed.wrapping_mul(1103515245).wrapping_add(12345);
        SEED.store(new_seed, Ordering::Relaxed);
        T::from(new_seed)
    }
}

use std::sync::Arc;